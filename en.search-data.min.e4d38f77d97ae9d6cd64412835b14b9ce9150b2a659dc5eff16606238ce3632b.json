[{"id":0,"href":"/articles/mle/lifecycle/","title":"ML Project Lifecycle","section":"Machine Learning Engineering","content":"ML Project Lifecycle #  The slides on this page are copied from \u0026ldquo;Introduction to Machine Learning in Production\u0026rdquo; course by DeepLearning.ai for educational purposes.  Scoping Stage #   Data Stage #   Modeling Stage #   Deployment Stage #   "},{"id":1,"href":"/articles/mle/ml_pipelines/","title":"ML Pipelines","section":"Machine Learning Engineering","content":"ML Pipelines #  Pipeline Orchestrators: infrastructure for automating, monitoring and maintaining model training and deployment.\nExamples: Airflow, Argo, Celery, Luigi, KubeFlow.\nTensorFlow Extended (TFX) #  Open-source end-to-end platform for deploying production ML pipelines.\n "},{"id":2,"href":"/articles/mle/lifecycle/scoping/","title":"Scoping Stage","section":"ML Project Lifecycle","content":"Scoping Stage #  Always separate a business problem from a technical solution.\nScoping Process:  mermaid.initialize({ \"flowchart\": { \"useMaxWidth\":true }, \"theme\": \"default\" } ) flowchart TD BP(Brainstorm business problems) AIS(Brainstorm AI solutions) ASS(Assess the feasibility and value) MIL(Determine milestones and budget) BP -- AIS AIS -- ASS ASS -- MIL Milestones \u0026amp; Resourcing #  Key specifications:\n ML metrics (accuracy, precision/recall, \u0026hellip;) Software metrics (latency, throughput, \u0026hellip;) Business metrics (revenue, \u0026hellip;) Resources needed (data, personnel, help from other teams, \u0026hellip;) Timeline  If unsure, consider benchmarking to other projects, or building a PoC first.\n"},{"id":3,"href":"/articles/mle/data/","title":"Data","section":"Machine Learning Engineering","content":"Data #  Data Inspection #   Identiry data sources Check how they are refreshed Consistency (formatting, data types) Outliers and errors  Responsilbe Data #   Bias in data User privacy:  Aggregation: replace unique values with summary Redaction: remove some data to create less complete picture   Compilance with GDPR and other regulations  Data Problems #  Data/Concept Drift #  Data changes:\n Trend and seasonality Distributiob of features changes Relative importance of features changes   World changes:\n Style changes Scope and processes change Competitors change Business expands to other geos       Drift Pace Slow Faster Really Fast     Ground truth change period months, years weeks days, hours, minutes   Retraining driven by - model improvements\n- better data\n- changes in software/systems - Declining model performance\n- model improvements\n- better data\n- changes in software/systems - Declining model performance\n- model improvements\n- better data\n- changes in software/systems   Labeling - Curated datasets\n- Crown-based - Direct feedback\n- Crowd-based - Direct feedback\n- Weak supervision          Sudden Problems #   Data collection problem: bad sensor, bad log data, moved or disabled sensor System problems: bad software update, loss of network connectivity, system down, bad credentials  Detecting Problems #   Label new training data to handle changing ground truth Validate data (schema, distribution) Monitor models  Data Labeling #  Process Feedback (direct labeling) #   Continuous dataset creation Lables evolve quicly Captures strong label signals Not always possible Individual design Tools: Logstash, fluentd, Google Cloud Logging, AWS ElasticSearch, Azure Monitor  Human Labeling #   slow difficult for many dataset expensive small datasets  Semi-supervied labeling #   can boost the accuracy cheap and fast small human-labeled data is mixed with large unlabeled data relies on some uniformity and clustering within feature space and performs label propogation  Active Learning #   algorithms for intelligent sampling data select the most informative points and label them only useful with contrained budgets when you can afford to label few data useful with imbalanced dataset: helps find rare classes  Techniques: margin sampling, cluster-based sampling, query-by-committee, region-based sampling, etc.\nWeak Supervision #   SME defines heuristics: labeling functions that generate noisy rough labels generative model de-noises and weights labels the labels are used to train model  Weak supervision framework: Snorkel.\nData Validation #  Typical workflow with TensorFlow Data Validation (TFDV)\n Infer schema (columns, constraints, domains) from training dataset and calculate statistics for each feature Check for anomalies (compare statistics) in evaluation dataset and adjust schema Store the schema Validate input serving data with the schema Monitor serving data statistics and track data drift  Data Preprocessing #  Data Preprocessing transforms raw data into a clean and training-ready dataset.\nPreprocessin Operations:\n Data cleansing Feature tuning (scaling, normalizing, \u0026hellip;) Representation transformation Feature extraction Feature construction  It is important that data preprocessing during training must also be applied correctly during serving  Text preprocessing techniques:\n stemming lemmatization TF-IDF n-grams embedding lookup  Image preprocessing techniques:\n clipping resizing cropping blug canny filters Sobel filter photometric distortions  Scaling #  Rescale numeric range to fit another range, that is more performant for ML model.\nExample: greyscale pixel [0, 255] is usually rescaled to [-1, 1].\nNormalization #  Transform any numeric range to [0, 1].\n  \\(X_{norm} = \\cfrac{X - X_{min}}{X_{max} - X_{min}}\\)  Standardization #  Z-score: number of standard deviations away from the mean.\n \\(X_{std} = \\cfrac{X - \\mu}{\\sigma}\\)  Bucketizing / Binning #  Bucketizing (Binning) is a grouping technique, that creates categories from a numeric range.\nFeature crossing #  Feature crossing: combining multiple features into one. Example: week + hour = hour of the week.\nOther techniques #  Dimensionality reduction in embeddings:\n Principal component analysis (PCA) t-Distribution stochastic neighbor embedding (t-SNE) Uniform manifold approximation and projection (UMAP)  Preprocessing Data at Scale #   Real-world models can be terabytes of data. Large-scale data processing frameworks should be used to handle such a volume. Consisnent trasformation between training and serving is crutial.  When do you transform? #      Pros Cons     Pre-process whole training dataset Run-once\nCompute on entire dataset Transformations reproduced at serving\nSlower iterations   Within a model Easy iterations\nTransform guarantees Expensive transforms\nLong model latency\nTransformations per batch: skew   Transform per batch Access to a single batch, not the full dataset\nScales better Deal with normalization and other \u0026ldquo;wide\u0026rdquo; transformations:\n- normalize by average withing a batch\n- precompute average and reuse it in batch processing    Prefetch transformation #  To optimize narrow transformations, the \u0026ldquo;prefetch\u0026rdquo; transformation can be used. It fetches the next batch of data ahead of time, while the previous batch is still processing. This approaches minimizes the idle time.\nFeature Selection #  Feature Space: n-dimensional space that has each feature of a dataset as an axis.\nTraining is learning the decision boundry in a feature space: a line in 2D, a surface in 3D, etc\u0026hellip; Feature Space Coverage: train/eval dataset should cover the same areas of Feature Space that the serving dataset does.\n same numerical ranges same classes similar characteristics for image data similar vocabulary, syntax and semantics for NLP data  Feature Selection:\n identify features that best represent the relationship remove features that don\u0026rsquo;t influence the outcome reduce the size of the feature space  Unsupervised In unsupervised feature selection method the target column is not considered.\nIt looks for correlation between features and removes features that highly correlate with some other ones.\n Supervised\nTarget column is not considered.\nIt selects features that contibute to the result the most.\n  Supervised methods:\n Filter methods Wrapper methods Embedded methods  Filter methods #   Correlation (between features and between features and the label) Univariate feature selection  To detect correlation the correlation matrix can be used.\nCorrelation tests:\n Pearson correlation: lenear relationship Kendall Tau Rank Correlation Coefficient: monotonic relationships and small sample size Spearman\u0026rsquo;s Rank Correlation Coefficient: monotonic relationsships  Other methods:\n Mutual information F-test Chi-Squared test  Wrapper methods #  Forward selection\n Start with 1 feature evaluate model performance when adding each of the additional features, one at a time Add the one that permorms best Repeat until there\u0026rsquo;s no improvement   Backward elimination\n Start with all features evaluate model performance when removing each of the included features, one at a time Remove the one that permorms worst Repeat until there\u0026rsquo;s no improvement   Recursive elimination\n Select a model for evaluating feature improtance (not all models are able to do that) Select the desired number of features Fit the model Rank features by importance Discard least important features Repeat until the desired number of features remains    Embedded methods #  Depends on the model you are using.\n L1 regularization Feature importance  "},{"id":4,"href":"/articles/mle/lifecycle/data/","title":"Data Stage","section":"ML Project Lifecycle","content":"Data Stage #  Types of Datasets #  Small Data vs. Big Data: generally, the threshold is \u0026ldquo;can a small team examine every single example in a reasonable time?\u0026rdquo;.\nSmall Data\n Every record can be manually examined Humans can label data Clean (true and consistent) labels are critical If the labels are inconsistent, it may be more worthy to fix that instead of collecting new data and overcomplicate a model   Big Data\n Manual labeling is nearly impossible Focus of data processes but problems of rear events in Big Data are pretty similar to the problems of Small Data, where accurate labeling is critical    Unstructured Data\n Humans can label more data Data Augmentation more likely to be helpful   Structured Data\n May be more difficult to obtain more data Human labeling may not be possible    Examples #      Unstructured Structured     Small Data Manufacturing visual inspection from 100 training examples Housing price prediction based on square footage, etc. from 50 examples   Big Data Speech recognition from 50 million examples Online shopping recommendations for 1 million users    Label Consistency #  Techniques:\n Standardizing: make an agreement between MLE, Subject Matter Experts and labelers. As well-defined as posible. Merging Classes: sometimes it\u0026rsquo;s more productive to treat some classes as one (e.g. \u0026ldquo;small scratch\u0026rdquo; and \u0026ldquo;deep scratch\u0026rdquo; -\u0026gt; \u0026ldquo;scratch\u0026rdquo;) Unknown/Borderline Class: introduce a separate class for ambiguous examples  To verify label consistency, the same data can be re-labeled by a few labelers or even by the same labeler after some time.\nGround Truth Definition #  It is critical to understand how is the Ground Truth defined. How objective it is?\nWhen the GT is externally defined, HLP (Human Level Performance) gives an estimate of irreducable error. But if the GT is defined by a human\u0026rsquo;s decision, it is the algorithm accuracy is generally \u0026ldquo;how close it is to the human\u0026rsquo;s opinion\u0026rdquo;.\nObtain New Data #  The earlier you start to iterate through model training cycle - the better.\nBrainstorm list of data sources:\n   Source Amount Cost Time     Owned 100 units $0 0   Crownsourced 1000 units $10000 14d   Pay for labels 100 units $6000 7d   Purchase data 1000 units $10000 1d    Don\u0026rsquo;t increase the dataset size more than 10x at a time. Otherwise, it can be really hard to predict how it will affect the model  Data Provenance and Data Linage #  Data Provenance: where the data comes from.\nData Linage: how to reproduce the same data through sequence of steps.\n"},{"id":5,"href":"/articles/mle/lifecycle/modeling/","title":"Modeling Stage","section":"ML Project Lifecycle","content":"Modeling #  AI system = Code + Data  Approaches to modeling\n Model-centric: improve the algorithm Data-centric: improve the data quality  Training Cycle #   mermaid.initialize({ \"flowchart\": { \"useMaxWidth\":true }, \"theme\": \"default\" } ) flowchart LR MHD(Model \\n + Hyperparameters \\n + Data) TR(Training) EA(Error Analysis) MHD -- TR TR -- EA EA -- MHD Why Low Average Error isn\u0026rsquo;t Good Enough #  Typical ML model challenges:\n Perform well on training dataset (avg training error) Perform well on dev/test sets Perform well on business metrics/project goals.\nIn most cases, this is not the same as item #2.  Priorities #  The model performance is measured by average error metrics over all possible requests. But in real world requests have priorities. So-called \u0026ldquo;disproportionally important\u0026rdquo; requests. From business perspective, the model performance requirements on such requests are higher.\n Example:\nIn web search engine there are requests like: \u0026ldquo;Apple pie recipe\u0026rdquo; or \u0026ldquo;Latest movies\u0026rdquo;. They are Informational and Transactional queries. Most likely, there\u0026rsquo;s no \u0026ldquo;best apple pie recipe\u0026rdquo;, so errors in ranking are forgivable.\nIn contrast, if user queries \u0026ldquo;YouTube\u0026rdquo; or \u0026ldquo;Reddit\u0026rdquo; they expect the correct result to be Top-1. Such queries are Navigational and have higher error cost.\n Performance on Key Slices #  Make sure to treat all slices of the dataset fairly. The model must not discriminate by gender, location, ethnicity or other protected attributes.\nRare Classes #  Skewed data distribution:\n 99% negative 1% positive  If some class is rare the model can just ignore it without a major effect on average accuracy.\nTo detect such a skew dataset the Confusion Matrix should be used instead of Accuracy.\n  \\(Precision = \\cfrac{TP}{TP \u0026#43; FP} \\qquad Recall = \\cfrac{TP}{TP \u0026#43; FN}\\)  Combining Precision and Recall - F1 score\n \\(F_1 = \\cfrac{2}{\\cfrac{1}{Precision} \u0026#43; \\cfrac{1}{Recall}}\\)  Establishing a Baseline Level of Performance #  Baseline helps indicate what might be possible. In some cases it also gives a sense of what is irreducable error (Bayes error). A baseline can be established per category.\nWays to establish a baseline:\n Human Level Performance (HLP) Search for results of similar projects Quick-and-dirty implementation Performance of older system  Error Analysis #  By manually expecting misclassified examples you may come up with additional attributes/tags that should be assigned to records in a dataset. For example, you found out that some audio clips have car noises in the background. Maybe it\u0026rsquo;s worth to add this column to the dataset and check:\n what fraction of errors has that tag? Of all data with that tag what fraction is misclassified? What fraction of all the data has that tag? How much room for improvement is there on data with that tag?  Baseline helps indicate how much room for improvement is there for certain category:\n(Baseline accuracy - Current accuracy) * Category frequency.\nFor categories you want to prioritize:\n Collect more data Use augmentation to get more data Improve data quality  Data Manipulations for Improving Performance #  Data Augmentation #  The goal is to create realistic examples that:\n the algorithm does poorly on humans (or other baseline) do well on  As a rule, Data Augmentation does not hurt the performance if:\n the model is large (low bias) The mapping x -\u0026gt; y is clear (1 vs I)  Data Augmentation is a good fit for unstructured data.\nAdding features #  For structured data it\u0026rsquo;s often more efficient to add new features to existent data instead of creating completely new data records.\nExperiment Tracking #  Things to track:\n Algorithm version Dataset used Hyperparameters Results  Desirable Features:\n Experiment reproducability Experiment results with metrics analysis Perhaps: resource consumption, visualization, etc\u0026hellip;  "},{"id":6,"href":"/articles/mle/lifecycle/deployment/","title":"Deployment Stage","section":"ML Project Lifecycle","content":"Model Deployment #  Key Challenges #  ML/Statistics Issues #    Data Drift: Input data evolved and a trained model does not interpret it properly anymore\n  Concept Drift: The \u0026ldquo;rules\u0026rdquo; of taking the decision evolved\n   Software Engineering Issues #   Realtime or Batch Cloud or Edge Resources Requirements Latency/Throughput requirements Logging Security and Privacy    Common Deployment Cases #   New product Automate/assist with manual task Replace previous ML system  Deployment modes #   Shadow mode  ML system works in parallel with current solution (manual or previous automated system) ML system\u0026rsquo;s decisions are not taken into account at this point Monitoring system compares results from both systems to estimate accuracy and probably collect more training data   Canary deployment  ML system works in parallel with current solution New system handles a small portion of traffic, e.g. 5% If there\u0026rsquo;s no degradation, the portion is gradually increased   Blue/Green Deployment  ML system works in parallel with current solution At some point a router in front of both systems switches all the traffic to the new system In case of degradation, the rollback is easy    Degree of Automation #   mermaid.initialize({ \"flowchart\": { \"useMaxWidth\":true }, \"theme\": \"default\" } ) flowchart LR HO[/Human Only/] SM[/Shadow Mode/] AA[/AI Assistance/] PA[/Partial Automation/] FA[/Full Automation/] HO---SM---AA---PA---FA \u0026ldquo;Human in the loop\u0026rdquo; deployments:\n AI Assistance: ML System highlights interesting input, but the decision is still taken by human Partial Automation: the decisions are taken by the ML System, but if it is not sure, it forwards the request to human. The approach if very useful to collect more training data when the accuracy is not good enough.  Monitoring #  To build a monitoring dashboard:\n Brainstorm the things that could go wrong Define the metrics that would reflect these problems  It is OK to start with a big number of metrics and remove some of them as you understand which are not representative.  Examples of Metrics #  Software Metrics #   Memory Compute Latency Throughput Server Load   Input Metrics #   Number of missing values Avg input audio length/volume Avg image brightness   Output Metrics #   Average output value Number of NULLs in output Number of times user retries the same request Number of times user refuse to use the system    "},{"id":7,"href":"/articles/yandex/","title":"Yandex Cloud","section":"Articles","content":"Yandex Cloud #  Resource Structure in Yandex Cloud #   As a rule:\n 1 cloud == 1 company 1 catalog == 1 project  Resource Manager is responsible for organising resources in a structure and assigning permissions.\nThere are 2 types of roles:\n primitive predefined roles: global roles through all the services  viewer editor admin   service roles: granular roles per each single service fygbyvghcgh Roles are inherited: editor on a Cloud can manage resources in any catalog of the Cloud editor on a Catalog can manage only the resources withing that Catalog  Users #  There are 3 types of users:\n Yandex ID Federative account, provided by a supported external authentication system, e.g. Microsoft ActiveDirectory Service account  Identity and Access Management service is responsible for checking permissions when a user performs any action:  VPC #  To group and isolate resources Virtual Private Cloud (VPC) is used.\nVPC can have unlimited number of Subnets. By default, for every VPC 3 subnets are created - for each Availability Zone. The subnets must not interfere in terms of IPv4 adresses. By default subnets do not have an access to the Internet. To enable it, you can:\n enable \u0026ldquo;NAT to Internet\u0026rdquo; option (in Preview), or set up your own NAT instance on Compute VM.\nIf a Compute VM has a public IP and is accessible from the Internet, and can access the Internet itself as well.  All resources within a VPC can access each other by internal addresses, no matter of which subnet they are assigned to.\nTo manually manage routing within a subnet, Routing Tables are used.\nSecurity Groups (in Preview) are defined in VPC and then can be assigned to network resources to controll firewall rules.\nCompute #  Compute service creates Virtual Machines (VM). VMs are created from image: an operating system (Ubuntu, CentOS, Windows, \u0026hellip;) or a preconfigured software (Postgres, NAT Instance, VPN Server, \u0026hellip;). Preemtible VMs can be turned off when the compute resources are requested by regular VMs. And then they restart when the resources are available again, without time guaranties. Good for testing, orchestrated batch jobs, or K8s cluster.\nSnapshot is a state of a filesystem at some point in time. It is replicated in all AZs and can be used to restore a replica of some VM.\nImage is similar to snapshot, but is more performant in terms of restore time, thanks to the way of how snapshots and images are implemented under the hood.\nVM Groups or Instance Groups are to create a set of VMs with similar configuration (resources, networking) by a template. They are capable of autoscaling basing on CPU load or other custom metric.\nAccess private VM via SSH #  There are several ways to achieve that:\n First SSH to a public instance (e.g. NAT instance) in the same VPC and then SSH to the private one Setup a VPN server to access the whole private network space Use a Serial Console; it is turned on/off in VM settings and should be turned off when not used.  Object Storage #  Access Control #  The access to objects is controlled by 3 mechanisms:\n IAM roles Access Control List (ACL) Bucket Policy  Here\u0026rsquo;s how the access is checked:  Which mechanism to choose? #   IAM roles are always checked, it covers basic use cases Bucket Policy allows to apply more complex criterias and conditions ACL allows to apply per-object rules  Pre-signed URL #  Pre-signed url is another option to provide access to object storage.\n the link is time-limitted (seconds to days) it includes the action that is permitted and a signature for authentication  "},{"id":8,"href":"/articles/mle/","title":"Machine Learning Engineering","section":"Articles","content":"  ML Project Lifecycle  Typical stages of ML Project from scope definition to production.   ML Pipelines  ML Pipelines # Pipeline Orchestrators: infrastructure for automating, monitoring and maintaining model training and deployment. Examples: Airflow, Argo, Celery, Luigi, KubeFlow. TensorFlow Extended (TFX) # Open-source end-to-end platform for deploying production ML pipelines.   Data  Data # Data Inspection # Identiry data sources Check how they are refreshed Consistency (formatting, data types) Outliers and errors Responsilbe Data # Bias in data User privacy: Aggregation: replace unique values with summary Redaction: remove some data to create less complete picture Compilance with GDPR and other regulations Data Problems # Data/Concept Drift # Data changes: Trend and seasonality Distributiob of features changes Relative importance of features changes World changes:   TensorFlow    Modeling  Modeling # Hyperparameters Tuning # Neural architecture search (NAS) is a technique for automating the design of artificial neural networks. Trainable paramters learned by the algorithm during taining e.g. weights of a neural network Hyperparameters set before launching the training not updated by the training itself e.g. learning rate, number of units in a dense layer Even in a small algorithm the number of tunable hyperparameters can be significant.   MLE Path    "},{"id":9,"href":"/articles/mle/modeling/","title":"Modeling","section":"Machine Learning Engineering","content":"Modeling #  Hyperparameters Tuning #  Neural architecture search (NAS) is a technique for automating the design of artificial neural networks.\nTrainable paramters\n learned by the algorithm during taining e.g. weights of a neural network   Hyperparameters\n set before launching the training not updated by the training itself e.g. learning rate, number of units in a dense layer    Even in a small algorithm the number of tunable hyperparameters can be significant. So the automation is key.\nExample framework for automated tuning is Keras Tuner.\n"},{"id":10,"href":"/articles/mle/data/storage/","title":"Storage","section":"Data","content":"Data Storage #  Feature Store #  Feature store is a central repository for storing documented, curated and access-controlled features.\nFeatures store enables the team to re-use shared features to avoid redundant work.\nKey points:\n managing feature data from a single person to an enterprise Scalable and performant access to features for training and serving provide consistent and point-in-time correct access enable discovery, documentation and insights  Offline feature procesing:\n feature engineering, like feature crossing data quality control offline storage discoverability  Online feature usage:\n low-latency access access to pre-computed features (computing aggregations and joins online is very time-consuming)  Data warehoues #  Key features:\n subject-oriented integrated (multiple data sources) non-volatile (access to history)  Data Lakes #   Aggregates raw data from multiple sources Structured and unsctructured Purpose of data may be not yet determined Doesn\u0026rsquo;t involve any data processing  "},{"id":11,"href":"/articles/mle/data/schema/","title":"Schema Development","section":"Data","content":"Schema Development #  Schema:\n Feature name Data type Required or optional Valency (for multivalue features) Domain or range Default value  Schema Environment #  Schema Environment means customizing schema basing on the current environment.\nExample: remove label column from serving environment.\n"},{"id":12,"href":"/articles/mle/data/lineage/","title":"Lineage and Provenance","section":"Data","content":"Data Lineage and Provenance #  To keep track on the data flow Metadata Store is used. It logs information about executions and artifacts.\nFor example, this is a high-level architecture of TensorFlow ML Metadata (MLMD): "},{"id":13,"href":"/articles/mle/tensorflow/transform/","title":"TensorFlow Transform","section":"TensorFlow","content":"TensorFlow Transform #   ExampleGen splits dataset StatisticsGen calculates statistics per feature SchemaGen infers the types of features ExampleValidator checks for data anomalies Transform performs feature engineering  Transform Inputs\n Data Schema   Transform Outputs\n Transformed Data Transform Graph    The result transform graph (tf.Graph) holds all necessary constants and transformations. The same chain of transformations can be easily applied at serving time.\nAnalizers Framework #  Analizers:\n Scaling  scale_to_z_score scale_to_0_1   Bucketizing  quantiles apply_buckets bucketize   Vocabulary  bag_of_words tfidf ngrams   Dimensionality Reduciton  pca    "}]