[{"id":0,"href":"/articles/mle/lifecycle/","title":"ML Project Lifecycle","section":"Machine Learning Engineering","content":"ML Project Lifecycle #  The slides on this page are copied from \u0026ldquo;Introduction to Machine Learning in Production\u0026rdquo; course by DeepLearning.ai for educational purposes.  Scoping Stage #   Data Stage #   Modeling Stage #   Deployment Stage #   "},{"id":1,"href":"/articles/mle/ml_pipelines/","title":"ML Pipelines","section":"Machine Learning Engineering","content":"ML Pipelines #  Pipeline Orchestrators: infrastructure for automating, monitoring and maintaining model training and deployment.\nExamples: Airflow, Argo, Celery, Luigi, KubeFlow.\nTensorFlow Extended (TFX) #  Open-source end-to-end platform for deploying production ML pipelines.\n "},{"id":2,"href":"/articles/mle/lifecycle/scoping/","title":"Scoping Stage","section":"ML Project Lifecycle","content":"Scoping Stage #  Always separate a business problem from a technical solution.\nScoping Process:  mermaid.initialize({ \"flowchart\": { \"useMaxWidth\":true }, \"theme\": \"default\" } ) flowchart TD BP(Brainstorm business problems) AIS(Brainstorm AI solutions) ASS(Assess the feasibility and value) MIL(Determine milestones and budget) BP -- AIS AIS -- ASS ASS -- MIL Milestones \u0026amp; Resourcing #  Key specifications:\n ML metrics (accuracy, precision/recall, \u0026hellip;) Software metrics (latency, throughput, \u0026hellip;) Business metrics (revenue, \u0026hellip;) Resources needed (data, personnel, help from other teams, \u0026hellip;) Timeline  If unsure, consider benchmarking to other projects, or building a PoC first.\n"},{"id":3,"href":"/articles/mle/data/","title":"Data","section":"Machine Learning Engineering","content":"Data #  Data Inspection #   Identiry data sources Check how they are refreshed Consistency (formatting, data types) Outliers and errors  Responsilbe Data #   Bias in data User privacy:  Aggregation: replace unique values with summary Redaction: remove some data to create less complete picture   Compilance with GDPR and other regulations  Data Problems #  Data/Concept Drift #  Data changes:\n Trend and seasonality Distributiob of features changes Relative importance of features changes   World changes:\n Style changes Scope and processes change Competitors change Business expands to other geos       Drift Pace Slow Faster Really Fast     Ground truth change period months, years weeks days, hours, minutes   Retraining driven by - model improvements\n- better data\n- changes in software/systems - Declining model performance\n- model improvements\n- better data\n- changes in software/systems - Declining model performance\n- model improvements\n- better data\n- changes in software/systems   Labeling - Curated datasets\n- Crown-based - Direct feedback\n- Crowd-based - Direct feedback\n- Weak supervision          Sudden Problems #   Data collection problem: bad sensor, bad log data, moved or disabled sensor System problems: bad software update, loss of network connectivity, system down, bad credentials  Detecting Problems #   Label new training data to handle changing ground truth Validate data (schema, distribution) Monitor models  Data Labeling #   Process Feedback (direct labeling)  Continuous dataset creation Lables avolve quicly Captures strong label signals Not always possible Individual design Tools: Logstash, fluentd, Google Cloud Logging, AWS ElasticSearch, Azure Monitor   Human Labeling  slow difficult for many dataset expensive small datasets   Semi-supervied labeling Active Learning Weak Supervision  Data Validation #  Typical workflow with TensorFlow Data Validation (TFDV)\n Infer schema (columns, constraints, domains) from training dataset and calculate statistics for each feature Check for anomalies (compare statistics) in evaluation dataset and adjust schema Store the schema Validate input serving data with the schema Monitor serving data statistics and track data drift  Data Preprocessing #  Data Preprocessing transforms raw data into a clean and training-ready dataset.\nPreprocessin Operations:\n Data cleansing Feature tuning (scaling, normalizing, \u0026hellip;) Representation transformation Feature extraction Feature construction  It is important that data preprocessing during training must also be applied correctly during serving  Text preprocessing techniques:\n stemming lemmatization TF-IDF n-grams embedding lookup  Image preprocessing techniques:\n clipping resizing cropping blug canny filters Sobel filter photometric distortions  Scaling #  Rescale numeric range to fit another range, that is more performant for ML model.\nExample: greyscale pixel [0, 255] is usually rescaled to [-1, 1].\nNormalization #  Transform any numeric range to [0, 1].\n  \\(X_{norm} = \\cfrac{X - X_{min}}{X_{max} - X_{min}}\\)  Standardization #  Z-score: number of standard deviations away from the mean.\n \\(X_{std} = \\cfrac{X - \\mu}{\\sigma}\\)  Bucketizing / Binning #  Bucketizing (Binning) is a grouping technique, that creates categories from a numeric range.\nFeature crossing #  Feature crossing: combining multiple features into one. Example: week + hour = hour of the week.\nOther techniques #  Dimensionality reduction in embeddings:\n Principal component analysis (PCA) t-Distribution stochastic neighbor embedding (t-SNE) Uniform manifold approximation and projection (UMAP)  Preprocessing Data at Scale #   Real-world models can be terabytes of data. Large-scale data processing frameworks should be used to handle such a volume. Consisnent trasformation between training and serving is crutial.  When do you transform? #      Pros Cons     Pre-process whole training dataset Run-once\nCompute on entire dataset Transformations reproduced at serving\nSlower iterations   Within a model Easy iterations\nTransform guarantees Expensive transforms\nLong model latency\nTransformations per batch: skew   Transform per batch Access to a single batch, not the full dataset\nScales better Deal with normalization and other \u0026ldquo;wide\u0026rdquo; transformations:\n- normalize by average withing a batch\n- precompute average and reuse it in batch processing    Prefetch transformation #  To optimize narrow transformations, the \u0026ldquo;prefetch\u0026rdquo; transformation can be used. It fetches the next batch of data ahead of time, while the previous batch is still processing. This approaches minimizes the idle time.\nFeature Selection #  Feature Space: n-dimensional space that has each feature of a dataset as an axis.\nTraining is learning the decision boundry in a feature space: a line in 2D, a surface in 3D, etc\u0026hellip; Feature Space Coverage: train/eval dataset should cover the same areas of Feature Space that the serving dataset does.\n same numerical ranges same classes similar characteristics for image data similar vocabulary, syntax and semantics for NLP data  Feature Selection:\n identify features that best represent the relationship remove features that don\u0026rsquo;t influence the outcome reduce the size of the feature space  Unsupervised In unsupervised feature selection method the target column is not considered.\nIt looks for correlation between features and removes features that highly correlate with some other ones.\n Supervised\nTarget column is not considered.\nIt selects features that contibute to the result the most.\n  Supervised methods:\n Filter methods Wrapper methods Embedded methods  Filter methods #   Correlation (between features and between features and the label) Univariate feature selection  To detect correlation the correlation matrix can be used.\nCorrelation tests:\n Pearson correlation: lenear relationship Kendall Tau Rank Correlation Coefficient: monotonic relationships and small sample size Spearman\u0026rsquo;s Rank Correlation Coefficient: monotonic relationsships  Other methods:\n Mutual information F-test Chi-Squared test  Wrapper methods #  Forward selection\n Start with 1 feature evaluate model performance when adding each of the additional features, one at a time Add the one that permorms best Repeat until there\u0026rsquo;s no improvement   Backward elimination\n Start with all features evaluate model performance when removing each of the included features, one at a time Remove the one that permorms worst Repeat until there\u0026rsquo;s no improvement   Recursive elimination\n Select a model for evaluating feature improtance (not all models are able to do that) Select the desired number of features Fit the model Rank features by importance Discard least important features Repeat until the desired number of features remains    Embedded methods #  Depends on the model you are using.\n L1 regularization Feature importance  "},{"id":4,"href":"/articles/mle/lifecycle/data/","title":"Data Stage","section":"ML Project Lifecycle","content":"Data Stage #  Types of Datasets #  Small Data vs. Big Data: generally, the threshold is \u0026ldquo;can a small team examine every single example in a reasonable time?\u0026rdquo;.\nSmall Data\n Every record can be manually examined Humans can label data Clean (true and consistent) labels are critical If the labels are inconsistent, it may be more worthy to fix that instead of collecting new data and overcomplicate a model   Big Data\n Manual labeling is nearly impossible Focus of data processes but problems of rear events in Big Data are pretty similar to the problems of Small Data, where accurate labeling is critical    Unstructured Data\n Humans can label more data Data Augmentation more likely to be helpful   Structured Data\n May be more difficult to obtain more data Human labeling may not be possible    Examples #      Unstructured Structured     Small Data Manufacturing visual inspection from 100 training examples Housing price prediction based on square footage, etc. from 50 examples   Big Data Speech recognition from 50 million examples Online shopping recommendations for 1 million users    Label Consistency #  Techniques:\n Standardizing: make an agreement between MLE, Subject Matter Experts and labelers. As well-defined as posible. Merging Classes: sometimes it\u0026rsquo;s more productive to treat some classes as one (e.g. \u0026ldquo;small scratch\u0026rdquo; and \u0026ldquo;deep scratch\u0026rdquo; -\u0026gt; \u0026ldquo;scratch\u0026rdquo;) Unknown/Borderline Class: introduce a separate class for ambiguous examples  To verify label consistency, the same data can be re-labeled by a few labelers or even by the same labeler after some time.\nGround Truth Definition #  It is critical to understand how is the Ground Truth defined. How objective it is?\nWhen the GT is externally defined, HLP (Human Level Performance) gives an estimate of irreducable error. But if the GT is defined by a human\u0026rsquo;s decision, it is the algorithm accuracy is generally \u0026ldquo;how close it is to the human\u0026rsquo;s opinion\u0026rdquo;.\nObtain New Data #  The earlier you start to iterate through model training cycle - the better.\nBrainstorm list of data sources:\n   Source Amount Cost Time     Owned 100 units $0 0   Crownsourced 1000 units $10000 14d   Pay for labels 100 units $6000 7d   Purchase data 1000 units $10000 1d    Don\u0026rsquo;t increase the dataset size more than 10x at a time. Otherwise, it can be really hard to predict how it will affect the model  Data Provenance and Data Linage #  Data Provenance: where the data comes from.\nData Linage: how to reproduce the same data through sequence of steps.\n"},{"id":5,"href":"/articles/mle/lifecycle/modeling/","title":"Modeling Stage","section":"ML Project Lifecycle","content":"Modeling #  AI system = Code + Data  Approaches to modeling\n Model-centric: improve the algorithm Data-centric: improve the data quality  Training Cycle #   mermaid.initialize({ \"flowchart\": { \"useMaxWidth\":true }, \"theme\": \"default\" } ) flowchart LR MHD(Model \\n + Hyperparameters \\n + Data) TR(Training) EA(Error Analysis) MHD -- TR TR -- EA EA -- MHD Why Low Average Error isn\u0026rsquo;t Good Enough #  Typical ML model challenges:\n Perform well on training dataset (avg training error) Perform well on dev/test sets Perform well on business metrics/project goals.\nIn most cases, this is not the same as item #2.  Priorities #  The model performance is measured by average error metrics over all possible requests. But in real world requests have priorities. So-called \u0026ldquo;disproportionally important\u0026rdquo; requests. From business perspective, the model performance requirements on such requests are higher.\n Example:\nIn web search engine there are requests like: \u0026ldquo;Apple pie recipe\u0026rdquo; or \u0026ldquo;Latest movies\u0026rdquo;. They are Informational and Transactional queries. Most likely, there\u0026rsquo;s no \u0026ldquo;best apple pie recipe\u0026rdquo;, so errors in ranking are forgivable.\nIn contrast, if user queries \u0026ldquo;YouTube\u0026rdquo; or \u0026ldquo;Reddit\u0026rdquo; they expect the correct result to be Top-1. Such queries are Navigational and have higher error cost.\n Performance on Key Slices #  Make sure to treat all slices of the dataset fairly. The model must not discriminate by gender, location, ethnicity or other protected attributes.\nRare Classes #  Skewed data distribution:\n 99% negative 1% positive  If some class is rare the model can just ignore it without a major effect on average accuracy.\nTo detect such a skew dataset the Confusion Matrix should be used instead of Accuracy.\n  \\(Precision = \\cfrac{TP}{TP \u0026#43; FP} \\qquad Recall = \\cfrac{TP}{TP \u0026#43; FN}\\)  Combining Precision and Recall - F1 score\n \\(F_1 = \\cfrac{2}{\\cfrac{1}{Precision} \u0026#43; \\cfrac{1}{Recall}}\\)  Establishing a Baseline Level of Performance #  Baseline helps indicate what might be possible. In some cases it also gives a sense of what is irreducable error (Bayes error). A baseline can be established per category.\nWays to establish a baseline:\n Human Level Performance (HLP) Search for results of similar projects Quick-and-dirty implementation Performance of older system  Error Analysis #  By manually expecting misclassified examples you may come up with additional attributes/tags that should be assigned to records in a dataset. For example, you found out that some audio clips have car noises in the background. Maybe it\u0026rsquo;s worth to add this column to the dataset and check:\n what fraction of errors has that tag? Of all data with that tag what fraction is misclassified? What fraction of all the data has that tag? How much room for improvement is there on data with that tag?  Baseline helps indicate how much room for improvement is there for certain category:\n(Baseline accuracy - Current accuracy) * Category frequency.\nFor categories you want to prioritize:\n Collect more data Use augmentation to get more data Improve data quality  Data Manipulations for Improving Performance #  Data Augmentation #  The goal is to create realistic examples that:\n the algorithm does poorly on humans (or other baseline) do well on  As a rule, Data Augmentation does not hurt the performance if:\n the model is large (low bias) The mapping x -\u0026gt; y is clear (1 vs I)  Data Augmentation is a good fit for unstructured data.\nAdding features #  For structured data it\u0026rsquo;s often more efficient to add new features to existent data instead of creating completely new data records.\nExperiment Tracking #  Things to track:\n Algorithm version Dataset used Hyperparameters Results  Desirable Features:\n Experiment reproducability Experiment results with metrics analysis Perhaps: resource consumption, visualization, etc\u0026hellip;  "},{"id":6,"href":"/articles/mle/lifecycle/deployment/","title":"Deployment Stage","section":"ML Project Lifecycle","content":"Model Deployment #  Key Challenges #  ML/Statistics Issues #    Data Drift: Input data evolved and a trained model does not interpret it properly anymore\n  Concept Drift: The \u0026ldquo;rules\u0026rdquo; of taking the decision evolved\n   Software Engineering Issues #   Realtime or Batch Cloud or Edge Resources Requirements Latency/Throughput requirements Logging Security and Privacy    Common Deployment Cases #   New product Automate/assist with manual task Replace previous ML system  Deployment modes #   Shadow mode  ML system works in parallel with current solution (manual or previous automated system) ML system\u0026rsquo;s decisions are not taken into account at this point Monitoring system compares results from both systems to estimate accuracy and probably collect more training data   Canary deployment  ML system works in parallel with current solution New system handles a small portion of traffic, e.g. 5% If there\u0026rsquo;s no degradation, the portion is gradually increased   Blue/Green Deployment  ML system works in parallel with current solution At some point a router in front of both systems switches all the traffic to the new system In case of degradation, the rollback is easy    Degree of Automation #   mermaid.initialize({ \"flowchart\": { \"useMaxWidth\":true }, \"theme\": \"default\" } ) flowchart LR HO[/Human Only/] SM[/Shadow Mode/] AA[/AI Assistance/] PA[/Partial Automation/] FA[/Full Automation/] HO---SM---AA---PA---FA \u0026ldquo;Human in the loop\u0026rdquo; deployments:\n AI Assistance: ML System highlights interesting input, but the decision is still taken by human Partial Automation: the decisions are taken by the ML System, but if it is not sure, it forwards the request to human. The approach if very useful to collect more training data when the accuracy is not good enough.  Monitoring #  To build a monitoring dashboard:\n Brainstorm the things that could go wrong Define the metrics that would reflect these problems  It is OK to start with a big number of metrics and remove some of them as you understand which are not representative.  Examples of Metrics #  Software Metrics #   Memory Compute Latency Throughput Server Load   Input Metrics #   Number of missing values Avg input audio length/volume Avg image brightness   Output Metrics #   Average output value Number of NULLs in output Number of times user retries the same request Number of times user refuse to use the system    "},{"id":7,"href":"/articles/mle/","title":"Machine Learning Engineering","section":"Articles","content":"  ML Project Lifecycle  Typical stages of ML Project from scope definition to production.   ML Pipelines  ML Pipelines # Pipeline Orchestrators: infrastructure for automating, monitoring and maintaining model training and deployment. Examples: Airflow, Argo, Celery, Luigi, KubeFlow. TensorFlow Extended (TFX) # Open-source end-to-end platform for deploying production ML pipelines.   Data  Data # Data Inspection # Identiry data sources Check how they are refreshed Consistency (formatting, data types) Outliers and errors Responsilbe Data # Bias in data User privacy: Aggregation: replace unique values with summary Redaction: remove some data to create less complete picture Compilance with GDPR and other regulations Data Problems # Data/Concept Drift # Data changes: Trend and seasonality Distributiob of features changes Relative importance of features changes World changes:   TensorFlow    "},{"id":8,"href":"/articles/mle/tensorflow/transform/","title":"TensorFlow Transform","section":"TensorFlow","content":"TensorFlow Transform #   ExampleGen splits dataset StatisticsGen calculates statistics per feature SchemaGen infers the types of features ExampleValidator checks for data anomalies Transform performs feature engineering  Transform Inputs\n Data Schema   Transform Outputs\n Transformed Data Transform Graph    The result transform graph (tf.Graph) holds all necessary constants and transformations. The same chain of transformations can be easily applied at serving time.\nAnalizers Framework #  Analizers:\n Scaling  scale_to_z_score scale_to_0_1   Bucketizing  quantiles apply_buckets bucketize   Vocabulary  bag_of_words tfidf ngrams   Dimensionality Reduciton  pca    "}]