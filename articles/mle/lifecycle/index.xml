<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ML Project Lifecycle on MinII KB</title><link>/articles/mle/lifecycle/</link><description>Recent content in ML Project Lifecycle on MinII KB</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 21 Feb 2022 15:52:08 +0300</lastBuildDate><atom:link href="/articles/mle/lifecycle/index.xml" rel="self" type="application/rss+xml"/><item><title>Scoping Stage</title><link>/articles/mle/lifecycle/scoping/</link><pubDate>Fri, 25 Feb 2022 16:48:37 +0300</pubDate><guid>/articles/mle/lifecycle/scoping/</guid><description>Scoping Stage # Always separate a business problem from a technical solution.
Scoping Process: mermaid.initialize({ "flowchart": { "useMaxWidth":true }, "theme": "default" } ) flowchart TD BP(Brainstorm business problems) AIS(Brainstorm AI solutions) ASS(Assess the feasibility and value) MIL(Determine milestones and budget) BP -- AIS AIS -- ASS ASS -- MIL Milestones &amp;amp; Resourcing # Key specifications:
ML metrics (accuracy, precision/recall, &amp;hellip;) Software metrics (latency, throughput, &amp;hellip;) Business metrics (revenue, &amp;hellip;) Resources needed (data, personnel, help from other teams, &amp;hellip;) Timeline If unsure, consider benchmarking to other projects, or building a PoC first.</description></item><item><title>Data Stage</title><link>/articles/mle/lifecycle/data/</link><pubDate>Fri, 25 Feb 2022 11:50:54 +0300</pubDate><guid>/articles/mle/lifecycle/data/</guid><description>Data Stage # Types of Datasets # Small Data vs. Big Data: generally, the threshold is &amp;ldquo;can a small team examine every single example in a reasonable time?&amp;rdquo;.
Small Data
Every record can be manually examined Humans can label data Clean (true and consistent) labels are critical If the labels are inconsistent, it may be more worthy to fix that instead of collecting new data and overcomplicate a model Big Data</description></item><item><title>Modeling Stage</title><link>/articles/mle/lifecycle/modeling/</link><pubDate>Thu, 24 Feb 2022 13:29:29 +0300</pubDate><guid>/articles/mle/lifecycle/modeling/</guid><description>Modeling # AI system = Code + Data Approaches to modeling
Model-centric: improve the algorithm Data-centric: improve the data quality Training Cycle # mermaid.initialize({ "flowchart": { "useMaxWidth":true }, "theme": "default" } ) flowchart LR MHD(Model \n + Hyperparameters \n + Data) TR(Training) EA(Error Analysis) MHD -- TR TR -- EA EA -- MHD Why Low Average Error isn&amp;rsquo;t Good Enough # Typical ML model challenges:</description></item><item><title>Deployment Stage</title><link>/articles/mle/lifecycle/deployment/</link><pubDate>Tue, 22 Feb 2022 15:55:39 +0300</pubDate><guid>/articles/mle/lifecycle/deployment/</guid><description>Model Deployment # Key Challenges # ML/Statistics Issues # Data Drift: Input data evolved and a trained model does not interpret it properly anymore
Concept Drift: The &amp;ldquo;rules&amp;rdquo; of taking the decision evolved
Software Engineering Issues # Realtime or Batch Cloud or Edge Resources Requirements Latency/Throughput requirements Logging Security and Privacy Common Deployment Cases # New product Automate/assist with manual task Replace previous ML system Deployment modes # Shadow mode ML system works in parallel with current solution (manual or previous automated system) ML system&amp;rsquo;s decisions are not taken into account at this point Monitoring system compares results from both systems to estimate accuracy and probably collect more training data Canary deployment ML system works in parallel with current solution New system handles a small portion of traffic, e.</description></item></channel></rss>